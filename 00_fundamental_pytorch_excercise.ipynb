{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubZEvo362Djq",
        "outputId": "6cba7012-ce41-4fd7-a2da-0d4241779c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0290, 0.4019, 0.2598],\n",
            "        [0.3666, 0.0583, 0.7006]], device='cuda:0') tensor([[0.0518, 0.4681, 0.6738],\n",
            "        [0.3315, 0.7837, 0.5631]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "seed = 1234\n",
        "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
        "torch.manual_seed(seed)\n",
        "tensor_a = torch.rand(2,3)\n",
        "tensor_a_on_gpu=tensor_a.to(device)\n",
        "tensor_b = torch.rand(2,3)\n",
        "tensor_b_on_gpu=tensor_b.to(device)\n",
        "print(tensor_a_on_gpu , tensor_b_on_gpu)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch_mm = torch.mm(tensor_a_on_gpu,tensor_b_on_gpu.T)\n",
        "print(torch_mm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT_QkMw25Xde",
        "outputId": "40bf8194-7f34-4142-b158-5da7bb9c6858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3647, 0.4709],\n",
            "        [0.5184, 0.5617]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch_mm.max() , torch_mm.min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i18p11OH5ukc",
        "outputId": "c5636d0f-160c-493e-f7a9-f02f740cb060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.5617, device='cuda:0'), tensor(0.3647, device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch_mm.argmin() , torch_mm.argmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC_merj96FEv",
        "outputId": "32ef8f99-6556-4daf-b68c-f4834c28a9e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0, device='cuda:0'), tensor(3, device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "tensor_x = torch.rand(size=(1,1,1,10))\n",
        "tensor_y = torch.squeeze(tensor_x)\n",
        "tensor_z = torch.squeeze(tensor_y)\n",
        "tensor_xx = torch.squeeze(tensor_z)\n",
        "print(tensor_xx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ThxifCb6Z3t",
        "outputId": "353f8723-2756-4b29-a4a2-2b1745145f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5349, 0.1988, 0.6592, 0.6569, 0.2328, 0.4251, 0.2071, 0.6297, 0.3653,\n",
            "        0.8513])\n"
          ]
        }
      ]
    }
  ]
}